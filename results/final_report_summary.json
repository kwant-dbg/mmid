{
  "project_title": "Multi-Modal Icon Vision System for Mobile UI Analysis",
  "completion_date": "November 08, 2025",
  "model": {
    "architecture": "YOLOv11 Nano",
    "parameters": "2.6M",
    "input_size": "640x640",
    "framework": "PyTorch 2.9 + Ultralytics 8.3"
  },
  "dataset": {
    "name": "Rico Mobile UI Dataset",
    "total_images": "72,219",
    "icon_classes": 26,
    "train_val_test_split": "70/20/10"
  },
  "performance": {
    "mAP50": "43.5%",
    "mAP50-95": "28.4%",
    "precision": "52.3%",
    "recall": "48.7%",
    "f1_score": "50.4%",
    "inference_speed": "0.9ms (1111 FPS)",
    "training_time": "1.9 hours"
  },
  "features_implemented": [
    "YOLOv11-based icon detection",
    "Multi-modal OCR integration (EasyOCR/Tesseract)",
    "Icon-text correlation and semantic mapping",
    "REST API for deployment",
    "Web-based demo interface",
    "Model export (ONNX, TensorRT, OpenVINO)",
    "Docker containerization",
    "Comprehensive evaluation metrics"
  ],
  "key_achievements": [
    "30% faster training with YOLOv11 vs YOLOv8",
    "5% higher mAP50 with optimizations",
    "Real-time inference (1111 FPS on GPU)",
    "Production-ready deployment pipeline",
    "Multi-modal UI understanding capability"
  ],
  "technologies_used": [
    "YOLOv11, PyTorch 2.9, Ultralytics 8.3",
    "EasyOCR, Tesseract OCR",
    "Flask 3.1, OpenCV 4.12",
    "Docker, ONNX Runtime",
    "NumPy 2.2, Matplotlib 3.10"
  ]
}